{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMaybzjqmSbq"
      },
      "source": [
        "# Vision Transformer (JAX + FLAX)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "y3BAPq7pEIGe"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import logging\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "try:\n",
        "    import flax\n",
        "except ImportError:\n",
        "    %pip install flax\n",
        "    import flax\n",
        "from flax import nnx\n",
        "try:\n",
        "    import jax\n",
        "except ImportError:\n",
        "    %pip install jax\n",
        "    import jax\n",
        "import jax.numpy as jnp\n",
        "try:\n",
        "    import optax\n",
        "except ImportError:\n",
        "    %pip install optax\n",
        "    import optax\n",
        "from jax.tree_util import tree_map\n",
        "import tqdm\n",
        "import torch\n",
        "from torchvision.transforms import transforms\n",
        "from torch.utils.data import Dataset, DataLoader, default_collate \n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Definitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "wuGiP_XXhuE7"
      },
      "outputs": [],
      "source": [
        "# Embedding Class\n",
        "class Embed(nnx.Module):\n",
        "\n",
        "    # Embedding = Patch Embedding + Cls Token + Pos Embedding\n",
        "    def __init__(self, config: dict, rngs: nnx.Rngs):\n",
        "        self.patch_size = config[\"patch_size\"]\n",
        "        self.embed_dim = config[\"embed_dim\"]\n",
        "        self.image_height = config[\"image_size\"]\n",
        "        self.image_width = config[\"image_size\"]\n",
        "        self.batch_size = config[\"batch\"]\n",
        "        self.patch_count = (self.image_height * self.image_width) // self.patch_size ** 2\n",
        "        self.rng = rngs\n",
        "\n",
        "        self.proj_layer = nnx.Conv(\n",
        "            in_features=3,\n",
        "            out_features=self.embed_dim,\n",
        "            kernel_size=(self.patch_size, self.patch_size),\n",
        "            strides=self.patch_size,\n",
        "            rngs=self.rng\n",
        "        )\n",
        "\n",
        "        self.cls_token = nnx.Param(\n",
        "            jax.random.normal(self.rng.params(), (1, 1, self.embed_dim))\n",
        "        )\n",
        "        \n",
        "        self.pos_embedding = nnx.Param(\n",
        "            jax.random.normal(self.rng.params(), (self.batch_size, self.patch_count + 1, self.embed_dim))\n",
        "        )\n",
        "\n",
        "    def __call__(self, x: jnp.array):\n",
        "        x = self.proj_layer(x)\n",
        "        x = jnp.reshape(x, (x.shape[0], (x.shape[1] * x.shape[2]), x.shape[3]))\n",
        "        self.cls_tokens = jnp.tile(self.cls_token, [self.batch_size, 1, 1])\n",
        "        x = jnp.concatenate([x, self.cls_tokens], axis=1)\n",
        "\n",
        "        x = x + self.pos_embedding\n",
        "        return x\n",
        "\n",
        "# Attention Head\n",
        "class AttentionHead(nnx.Module):\n",
        "    def __init__(self, embed_dim: int, attention_head_size: int, bias=True):\n",
        "        self.embed_dim= embed_dim\n",
        "        self.attention_head_size = attention_head_size\n",
        "\n",
        "        # Query, Key and Value Weight matrices\n",
        "        self.q_w = nnx.Linear(in_features=self.embed_dim, out_features=self.attention_head_size, use_bias=True, rngs=nnx.Rngs(0))\n",
        "        self.k_w = nnx.Linear(in_features=self.embed_dim, out_features=self.attention_head_size, use_bias=True, rngs=nnx.Rngs(0))\n",
        "        self.v_w = nnx.Linear(in_features=self.embed_dim, out_features=self.attention_head_size, use_bias=True, rngs=nnx.Rngs(0))\n",
        "\n",
        "    def __call__(self, x: jnp.array):\n",
        "        q_x, k_x, v_x = self.q_w(x), self.k_w(x), self.v_w(x)\n",
        "\n",
        "        # Calculate QK^T/sqrt(dk)\n",
        "        attn_out = jnp.matmul(q_x, jnp.matrix_transpose(k_x)) / jnp.sqrt(self.attention_head_size)\n",
        "        # Apply softmax\n",
        "        softmax_out = nnx.softmax(attn_out)\n",
        "        # Obtain the attention value with value\n",
        "        attn_value = jnp.matmul(softmax_out, v_x)\n",
        "\n",
        "        return attn_value\n",
        "\n",
        "# Multiheadattention\n",
        "class MultiHeadAttention(nnx.Module):\n",
        "    def __init__(self, config: dict):\n",
        "        self.embed_dim = config[\"embed_dim\"]\n",
        "        self.num_of_heads = config[\"num_of_heads\"]\n",
        "\n",
        "        self.attn_head_size = self.embed_dim // self.num_of_heads\n",
        "        self.all_head_size = self.attn_head_size * self.num_of_heads\n",
        "\n",
        "        self.heads = nnx.List()\n",
        "\n",
        "        for _ in range(self.num_of_heads):\n",
        "            self.attn_head = AttentionHead(\n",
        "                embed_dim=self.embed_dim,\n",
        "                attention_head_size=self.attn_head_size\n",
        "            )\n",
        "            self.heads.append(self.attn_head)\n",
        "        \n",
        "        self.linear_proj = nnx.Linear(in_features=self.all_head_size, out_features=self.embed_dim, use_bias=True, rngs=nnx.Rngs(0))\n",
        "        self.dropout = nnx.Dropout(0.3, rngs=nnx.Rngs(0))\n",
        "\n",
        "    def __call__(self, x: jnp.array):\n",
        "        attn_outputs = [head(x) for head in self.heads]\n",
        "\n",
        "        concat_output = jnp.concatenate(attn_outputs, axis=-1)\n",
        "        proj_output = self.linear_proj(concat_output)\n",
        "        proj_output = self.dropout(x)\n",
        "\n",
        "        return proj_output\n",
        "\n",
        "# MLP class\n",
        "class MLP(nnx.Module):\n",
        "    def __init__(self, config: dict):\n",
        "        self.embed_dim = config[\"embed_dim\"]\n",
        "        self.intermediate_size = config[\"intermediate_size\"]\n",
        "\n",
        "        self.linear1 = nnx.Linear(in_features=self.embed_dim, out_features=self.intermediate_size, use_bias=True, rngs=nnx.Rngs(0))\n",
        "        self.linear2= nnx.Linear(in_features=self.intermediate_size, out_features=self.embed_dim, use_bias=True, rngs=nnx.Rngs(0))\n",
        "        self.dropout = nnx.Dropout(0.3, rngs=nnx.Rngs(0))\n",
        "\n",
        "    def __call__(self, x: jnp.array):\n",
        "        x = self.linear1(x)\n",
        "        x = nnx.relu(x)\n",
        "        x = self.linear2(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Single block\n",
        "class Block(nnx.Module):\n",
        "    def __init__(self, config: dict):\n",
        "        self.config = config\n",
        "        self.embed_dim = config[\"embed_dim\"]\n",
        "        self.norm = nnx.BatchNorm(num_features=self.embed_dim, rngs=nnx.Rngs(0))\n",
        "        self.mha = MultiHeadAttention(self.config)\n",
        "        self.mlp = MLP(self.config)\n",
        "\n",
        "    def __call__(self, x: jnp.array):\n",
        "        norm_out = self.norm(x)\n",
        "        attn_out = self.mha(x)\n",
        "        attn_out = x + attn_out\n",
        "        norm_out = self.norm(attn_out)\n",
        "        mlp_out = self.mlp(norm_out)\n",
        "        block_out = mlp_out + norm_out\n",
        "\n",
        "        return block_out\n",
        "\n",
        "# Encoder\n",
        "class Encoder(nnx.Module):\n",
        "    def __init__(self, config: dict):\n",
        "        self.config = config\n",
        "        self.num_of_blocks = config[\"num_of_blocks\"]\n",
        "\n",
        "        self.blocks = nnx.List()\n",
        "\n",
        "        for _ in range(self.num_of_blocks):\n",
        "            block = Block(self.config)\n",
        "            self.blocks.append(block)\n",
        "\n",
        "\n",
        "    def __call__(self, x: jnp.array):\n",
        "        all_attns = []\n",
        "\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "        return x\n",
        "\n",
        "# ViT Classifier\n",
        "class ViT(nnx.Module):\n",
        "    def __init__(self, config: dict):\n",
        "        self.embed_layer = Embed(config, nnx.Rngs(0))\n",
        "        self.encoder = Encoder(config)\n",
        "        self.classifier = nnx.Linear(in_features=config[\"embed_dim\"], out_features=config[\"no_of_classes\"], use_bias=True, rngs=nnx.Rngs(0))\n",
        "\n",
        "    def __call__(self, x: jnp.array):\n",
        "        x = self.embed_layer(x)\n",
        "        x = self.encoder(x)\n",
        "        x = self.classifier(x[:, 0, :])\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## PyTorch Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FolderDataset(Dataset):\n",
        "    \"\"\"\n",
        "        A Custom dataset class inheriting the PyTorch Dataset Class.\n",
        "        Receives the the master directory as input which contains the images of different classes in different folders.\n",
        "    \"\"\"\n",
        "    def __init__(self, parent_dir: str, image_size: tuple, resize: bool, train: bool, train_size: float):\n",
        "        self.parent_dir = parent_dir\n",
        "        self.paths = list(Path(parent_dir).glob(\"*/*.jpg\"))\n",
        "        self.train = train\n",
        "        self.train_size = train_size\n",
        "        self.resize = resize\n",
        "        self.image_size = image_size\n",
        "\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.PILToTensor(),\n",
        "            transforms.Lambda(lambda x: x.permute(1, 2, 0))\n",
        "        ])\n",
        "\n",
        "        if self.train:\n",
        "            self.paths = self.paths[:int(self.train_size * len(self.paths))]\n",
        "        else:\n",
        "            self.paths = self.paths[int(self.train_size * len(self.paths)):]\n",
        "\n",
        "        self.classes, self.class_to_idx = self.get_class_class_idx()\n",
        "    \n",
        "\n",
        "    def get_class_class_idx(self):\n",
        "        \"\"\"\n",
        "            Takes in the master directory that contains the subdirectories with the images.\n",
        "        \"\"\"\n",
        "        classes = os.listdir(self.parent_dir)\n",
        "        class_idx = {}\n",
        "\n",
        "        for i, c in enumerate(classes):\n",
        "            class_idx[c] = i\n",
        "        \n",
        "        return classes, class_idx\n",
        "\n",
        "    def load_image(self, index: int):\n",
        "        image_path = self.paths[index]\n",
        "        image = Image.open(image_path)\n",
        "\n",
        "        if self.resize:\n",
        "            image = image.resize((self.image_size))\n",
        "    \n",
        "        return self.transform(image)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "\n",
        "    def __getitem__(self, index: int):\n",
        "        image = self.load_image(index)\n",
        "        class_name = self.paths[index].parent.name\n",
        "        class_idx = torch.tensor(self.class_to_idx[class_name], dtype=torch.int32)\n",
        "\n",
        "        return image, class_idx\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def loss_fn(model: nnx.Module, images: jnp.array, labels: jnp.array):\n",
        "    logits = model(images)\n",
        "    loss = optax.losses.softmax_cross_entropy_with_integer_labels(logits, labels).mean()\n",
        "    return loss, logits\n",
        "\n",
        "@nnx.jit\n",
        "def train_step(model: nnx.Module, images: jnp.array, labels: jnp.array, optim: nnx.Optimizer):\n",
        "    grad_fn = nnx.value_and_grad(loss_fn, has_aux=True)\n",
        "    (loss, logits), grads = grad_fn(model, images, labels)\n",
        "    \n",
        "    optim.update(model, grads)\n",
        "    \n",
        "    return loss\n",
        "\n",
        "@nnx.jit\n",
        "def eval_step(model: nnx.Module, images: jnp.array, labels: jnp.array, eval_metrics: nnx.MultiMetric):\n",
        "    loss, logits = loss_fn(model, images, labels)\n",
        "    eval_metrics.update(\n",
        "        loss=loss,\n",
        "        logits=logits,\n",
        "        labels=labels\n",
        "    )\n",
        "\n",
        "def train_one_epoch(model: nnx.Module, train_dataloader: Dataloader, epoch: int, epochs: int, optim: nnx.Optimizer, train_history: dict):\n",
        "    tqdm_bar_format = \"{desc}[{n_fmt}/{total_fmt}]{postfix} [{elapsed}<{remaining}]\"\n",
        "    total_steps = len(train_dataloader)\n",
        "    \n",
        "    model.train()\n",
        "\n",
        "    with tqdm.tqdm(\n",
        "        desc=f\"[Train] epoch: {epoch} / {epochs}, \",\n",
        "        total=total_steps,\n",
        "        bar_format=tqdm_bar_format,\n",
        "        leave=True\n",
        "    ) as pbar:\n",
        "        for images, labels in train_dataloader:\n",
        "            loss = train_step(model, images, labels, optim)\n",
        "            train_history[\"train_loss\"].append(loss.item())\n",
        "            pbar.set_postfix({\"loss\": loss.item()})\n",
        "            pbar.update(1)\n",
        "\n",
        "def eval_model(model: nnx.Module, val_dataloader: Dataloader, epoch: int, eval_metrics: nnx.MultiMetric, val_history: dict):\n",
        "    model.eval()\n",
        "    eval_metrics.reset() \n",
        "\n",
        "    for val_images, val_labels in val_dataloader:\n",
        "        loss = eval_step(model, val_images, val_labels, eval_metrics)\n",
        "\n",
        "    for metric, value in eval_metrics.compute().items():\n",
        "        val_history[f\"val_{metric}\"].append(value)\n",
        "    \n",
        "    print(f\"[Val] epoch: {epoch + 1} / {epochs}\")\n",
        "    print(f\"Loss: {val_history['val_loss'][-1]: .4f}\")\n",
        "    print(f\"Accuracy: {val_history['val_accuracy'][-1]: .4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Train] epoch: 0 / 100, [7/357], loss=1.9 [00:20<17:19] \n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[29], line 83\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m# Training loop\u001b[39;00m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m---> 83\u001b[0m     train_one_epoch(model, train_dataloader, epoch, epochs, optim, train_history)\n\u001b[0;32m     84\u001b[0m     eval_model(model, val_dataloader, epoch, eval_metrics, val_history)\n",
            "Cell \u001b[1;32mIn[28], line 36\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(model, train_dataloader, epoch, epochs, optim, train_history)\u001b[0m\n\u001b[0;32m     28\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtqdm(\n\u001b[0;32m     31\u001b[0m     desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Train] epoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m / \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     32\u001b[0m     total\u001b[38;5;241m=\u001b[39mtotal_steps,\n\u001b[0;32m     33\u001b[0m     bar_format\u001b[38;5;241m=\u001b[39mtqdm_bar_format,\n\u001b[0;32m     34\u001b[0m     leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     35\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[1;32m---> 36\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m train_dataloader:\n\u001b[0;32m     37\u001b[0m         loss \u001b[38;5;241m=\u001b[39m train_step(model, images, labels, optim)\n\u001b[0;32m     38\u001b[0m         train_history[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n",
            "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    714\u001b[0m ):\n",
            "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    763\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 764\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    766\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
            "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
            "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
            "Cell \u001b[1;32mIn[27], line 52\u001b[0m, in \u001b[0;36mFolderDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[1;32m---> 52\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_image(index)\n\u001b[0;32m     53\u001b[0m     class_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpaths[index]\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mname\n\u001b[0;32m     54\u001b[0m     class_idx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_to_idx[class_name], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint32)\n",
            "Cell \u001b[1;32mIn[27], line 44\u001b[0m, in \u001b[0;36mFolderDataset.load_image\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     41\u001b[0m image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(image_path)\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresize:\n\u001b[1;32m---> 44\u001b[0m     image \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mresize((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_size))\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(image)\n",
            "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\PIL\\Image.py:2356\u001b[0m, in \u001b[0;36mImage.resize\u001b[1;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[0;32m   2344\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2345\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduce(factor, box\u001b[38;5;241m=\u001b[39mreduce_box)\n\u001b[0;32m   2346\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduce)\n\u001b[0;32m   2347\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m Image\u001b[38;5;241m.\u001b[39mreduce(\u001b[38;5;28mself\u001b[39m, factor, box\u001b[38;5;241m=\u001b[39mreduce_box)\n\u001b[0;32m   2348\u001b[0m         )\n\u001b[0;32m   2349\u001b[0m         box \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2350\u001b[0m             (box[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_x,\n\u001b[0;32m   2351\u001b[0m             (box[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_y,\n\u001b[0;32m   2352\u001b[0m             (box[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_x,\n\u001b[0;32m   2353\u001b[0m             (box[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_y,\n\u001b[0;32m   2354\u001b[0m         )\n\u001b[1;32m-> 2356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mim\u001b[38;5;241m.\u001b[39mresize(size, resample, box))\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    \n",
        "    # Custom collate function\n",
        "    def numpy_collate(batch):\n",
        "        return tree_map(jnp.asarray, default_collate(batch))\n",
        "\n",
        "    # Create Dataset\n",
        "    train_data = FolderDataset(\n",
        "        parent_dir=\"D:/hacks/csiro-biomass/dataset\",\n",
        "        train=True,\n",
        "        train_size=0.8,\n",
        "        resize=True,\n",
        "        image_size=(224,224),\n",
        "    )\n",
        "    val_data = FolderDataset(\n",
        "        parent_dir=\"D:/hacks/csiro-biomass/dataset\",\n",
        "        train=False,\n",
        "        train_size=0.8,\n",
        "        resize=True,\n",
        "        image_size=(224,224),\n",
        "    )\n",
        "\n",
        "    # Create Dataloaders\n",
        "    train_dataloader = DataLoader(\n",
        "        train_data,\n",
        "        batch_size=16,\n",
        "        shuffle=True,\n",
        "        collate_fn=numpy_collate\n",
        "    )\n",
        "\n",
        "    val_dataloader = DataLoader(\n",
        "        val_data,\n",
        "        batch_size=16,\n",
        "        shuffle=False,\n",
        "        collate_fn=numpy_collate\n",
        "    )\n",
        "\n",
        "    # Misc Settings\n",
        "    \n",
        "\n",
        "    lr_0 = 0.001\n",
        "    lr_f = 1e-5\n",
        "    momemtum = 0.8\n",
        "    epochs = 100\n",
        "    total_steps = len(train_data)\n",
        "    \n",
        "    config_dict = {\n",
        "        \"patch_size\": 16,\n",
        "        \"embed_dim\": 128,\n",
        "        \"image_size\": 224,\n",
        "        \"batch\": 16,\n",
        "        \"no_of_classes\": 10,\n",
        "        \"num_of_blocks\": 6,\n",
        "        \"num_of_heads\": 12,\n",
        "        \"intermediate_size\": 4 * 128,\n",
        "    }\n",
        "\n",
        "    train_history = {\n",
        "        \"train_loss\": [], \n",
        "    }\n",
        "\n",
        "    val_history = {\n",
        "        \"val_loss\": [],\n",
        "        \"val_accuracy\": []\n",
        "    }\n",
        "    \n",
        "    # Model Settings\n",
        "    model = ViT(config_dict)\n",
        "\n",
        "    lr_schedule = optax.linear_schedule(lr_0, lr_f, epochs * total_steps)\n",
        "\n",
        "    optim = nnx.Optimizer(\n",
        "        model, optax.adam(lr_schedule), wrt=nnx.Param\n",
        "    )\n",
        "\n",
        "    eval_metrics = nnx.MultiMetric(\n",
        "        loss = nnx.metrics.Average(\"loss\"),\n",
        "        accuracy = nnx.metrics.Accuracy(),\n",
        "    )\n",
        "    \n",
        "    # Training loop\n",
        "    for epoch in range(epochs):\n",
        "        train_one_epoch(model, train_dataloader, epoch, epochs, optim, train_history)\n",
        "        eval_model(model, val_dataloader, epoch, eval_metrics, val_history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Legacy code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the loss function\n",
        "def loss_fn(model: nnx.Module, images: jnp.array, labels: jnp.array):\n",
        "    logits = model(images)\n",
        "    loss = optax.losses.softmax_cross_entropy_with_integer_labels(logits, labels).mean()\n",
        "    return loss, logits\n",
        "\n",
        "# Define Training and validation step\n",
        "def train_step(model: nnx.Module, optim: nnx.Optimizer, images: jax.Array, labels: jax.Array):\n",
        "\n",
        "    grad_fn = nnx.value_and_grad(loss_fn, has_aux=True)\n",
        "    (loss, logits), grads = grad_fn(model, images, labels)\n",
        "    \n",
        "    optim.update(model, grads)\n",
        "    return loss\n",
        "\n",
        "def eval_step(model: nnx.Module, images: jax.Array, labels: jax.Array, eval_metrics: nnx.MultiMetric):\n",
        "    \n",
        "    loss, logits = loss_fn(model, images, labels)\n",
        "    eval_metrics.update(\n",
        "        loss=loss,\n",
        "        logits=logits,\n",
        "        labels=labels\n",
        "    )\n",
        "\n",
        "train_step = nnx.jit(train_step)\n",
        "eval_step = nnx.jit(eval_step)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "eval_metrics = nnx.MultiMetric(\n",
        "    loss = nnx.metrics.Average(\"loss\"),\n",
        "    accuracy = nnx.metrics.Accuracy(),\n",
        ")\n",
        "\n",
        "train_history = {\n",
        "    \"train_loss\": [], \n",
        "}\n",
        "\n",
        "val_history = {\n",
        "    \"val_loss\": [],\n",
        "    \"val_accuracy\": []\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure optimizer, loss, metrics\n",
        "\n",
        "tqdm_bar_format = \"{desc}[{n_fmt}/{total_fmt}]{postfix} [{elapsed}<{remaining}]\"\n",
        "\n",
        "lr_0 = 0.001\n",
        "lr_f = 1e-5\n",
        "momemtum = 0.8\n",
        "epochs = 100\n",
        "total_steps = x_train.shape[0] // 16\n",
        "\n",
        "config_dict = {\n",
        "    \"patch_size\": 4,\n",
        "    \"embed_dim\": 128,\n",
        "    \"image_size\": 32,\n",
        "    \"batch\": 16,\n",
        "    \"no_of_classes\": 10,\n",
        "    \"num_of_blocks\": 6,\n",
        "    \"num_of_heads\": 12,\n",
        "    \"intermediate_size\": 4 * 128,\n",
        "}\n",
        "train_history = {\n",
        "    \"train_loss\": [], \n",
        "}\n",
        "\n",
        "\n",
        "model = ViT(config_dict)\n",
        "\n",
        "lr_schedule = optax.linear_schedule(lr_0, lr_f, epochs * total_steps)\n",
        "\n",
        "optim = nnx.Optimizer(\n",
        "    model, optax.adam(lr_schedule), wrt=nnx.Param\n",
        ")\n",
        "\n",
        "def train_one_epoch(epoch: int):\n",
        "    model.train()\n",
        "\n",
        "    with tqdm.tqdm(\n",
        "        desc=f\"[Train] epoch: {epoch} / {epochs}, \",\n",
        "        total=total_steps,\n",
        "        bar_format=tqdm_bar_format,\n",
        "        leave=True\n",
        "    ) as pbar:\n",
        "        for images, labels in next(train_dataloader):\n",
        "            loss = train_step(model, optim, images, labels)\n",
        "            train_history[\"train_loss\"].append(loss.item())\n",
        "            pbar.set_postfix({\"loss\": loss.item()})\n",
        "            pbar.update(1)\n",
        "    \n",
        "def eval_model(epoch: int):\n",
        "    model.eval()\n",
        "    eval_metrics.reset()\n",
        "\n",
        "    for val_images, val_labels in next(val_dataloader):\n",
        "        loss = eval_step(model, val_images, val_labels, eval_metrics)\n",
        "    \n",
        "    for metric, value in eval_metrics.compute().items():\n",
        "        val_history[f\"val_{metric}\"].append(value)\n",
        "\n",
        "    print(f\"[Val] epoch: {epoch + 1} / {epochs}\")\n",
        "    print(f\"Loss: {val_history['val_loss'][-1]:0.4f}\")\n",
        "    print(f\"Accuracy: {val_history['val_accuracy'][-1]:0.4f}\")\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "625"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(train_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for epoch in range(epochs):\n",
        "    train_one_epoch(epoch)\n",
        "    eval_model(epoch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
