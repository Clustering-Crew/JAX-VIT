{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMaybzjqmSbq"
      },
      "source": [
        "# Vision Transformer (JAX + FLAX)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "y3BAPq7pEIGe"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import logging\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "try:\n",
        "    import flax\n",
        "except ImportError:\n",
        "    %pip install flax\n",
        "    import flax\n",
        "from flax import nnx\n",
        "try:\n",
        "    import jax\n",
        "except ImportError:\n",
        "    %pip install jax\n",
        "    import jax\n",
        "import jax.numpy as jnp\n",
        "try:\n",
        "    import optax\n",
        "except ImportError:\n",
        "    %pip install optax\n",
        "    import optax\n",
        "from jax.tree_util import tree_map\n",
        "import tqdm\n",
        "import torch\n",
        "from torchvision.transforms import transforms\n",
        "from torch.utils.data import Dataset, DataLoader, default_collate \n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Definitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "wuGiP_XXhuE7"
      },
      "outputs": [],
      "source": [
        "# Embedding Class\n",
        "class Embed(nnx.Module):\n",
        "\n",
        "    # Embedding = Patch Embedding + Cls Token + Pos Embedding\n",
        "    def __init__(self, config: dict, rngs: nnx.Rngs):\n",
        "        self.patch_size = config[\"patch_size\"]\n",
        "        self.embed_dim = config[\"embed_dim\"]\n",
        "        self.image_height = config[\"image_size\"]\n",
        "        self.image_width = config[\"image_size\"]\n",
        "        self.batch_size = config[\"batch\"]\n",
        "        self.patch_count = (self.image_height * self.image_width) // self.patch_size ** 2\n",
        "        self.rng = rngs\n",
        "\n",
        "        self.proj_layer = nnx.Conv(\n",
        "            in_features=3,\n",
        "            out_features=self.embed_dim,\n",
        "            kernel_size=(self.patch_size, self.patch_size),\n",
        "            strides=self.patch_size,\n",
        "            rngs=self.rng\n",
        "        )\n",
        "\n",
        "        self.cls_token = nnx.Param(\n",
        "            jax.random.normal(self.rng.params(), (1, 1, self.embed_dim))\n",
        "        )\n",
        "        \n",
        "        self.pos_embedding = nnx.Param(\n",
        "            jax.random.normal(self.rng.params(), (self.batch_size, self.patch_count + 1, self.embed_dim))\n",
        "        )\n",
        "\n",
        "    def __call__(self, x: jnp.array):\n",
        "        x = self.proj_layer(x)\n",
        "        x = jnp.reshape(x, (x.shape[0], (x.shape[1] * x.shape[2]), x.shape[3]))\n",
        "        self.cls_tokens = jnp.tile(self.cls_token, [self.batch_size, 1, 1])\n",
        "        x = jnp.concatenate([x, self.cls_tokens], axis=1)\n",
        "\n",
        "        x = x + self.pos_embedding\n",
        "        return x\n",
        "\n",
        "# Attention Head\n",
        "class AttentionHead(nnx.Module):\n",
        "    def __init__(self, embed_dim: int, attention_head_size: int, bias=True):\n",
        "        self.embed_dim= embed_dim\n",
        "        self.attention_head_size = attention_head_size\n",
        "\n",
        "        # Query, Key and Value Weight matrices\n",
        "        self.q_w = nnx.Linear(in_features=self.embed_dim, out_features=self.attention_head_size, use_bias=True, rngs=nnx.Rngs(0))\n",
        "        self.k_w = nnx.Linear(in_features=self.embed_dim, out_features=self.attention_head_size, use_bias=True, rngs=nnx.Rngs(0))\n",
        "        self.v_w = nnx.Linear(in_features=self.embed_dim, out_features=self.attention_head_size, use_bias=True, rngs=nnx.Rngs(0))\n",
        "\n",
        "    def __call__(self, x: jnp.array):\n",
        "        q_x, k_x, v_x = self.q_w(x), self.k_w(x), self.v_w(x)\n",
        "\n",
        "        # Calculate QK^T/sqrt(dk)\n",
        "        attn_out = jnp.matmul(q_x, jnp.matrix_transpose(k_x)) / jnp.sqrt(self.attention_head_size)\n",
        "        # Apply softmax\n",
        "        softmax_out = nnx.softmax(attn_out)\n",
        "        # Obtain the attention value with value\n",
        "        attn_value = jnp.matmul(softmax_out, v_x)\n",
        "\n",
        "        return attn_value\n",
        "\n",
        "# Multiheadattention\n",
        "class MultiHeadAttention(nnx.Module):\n",
        "    def __init__(self, config: dict):\n",
        "        self.embed_dim = config[\"embed_dim\"]\n",
        "        self.num_of_heads = config[\"num_of_heads\"]\n",
        "\n",
        "        self.attn_head_size = self.embed_dim // self.num_of_heads\n",
        "        self.all_head_size = self.attn_head_size * self.num_of_heads\n",
        "\n",
        "        self.heads = nnx.List()\n",
        "\n",
        "        for _ in range(self.num_of_heads):\n",
        "            self.attn_head = AttentionHead(\n",
        "                embed_dim=self.embed_dim,\n",
        "                attention_head_size=self.attn_head_size\n",
        "            )\n",
        "            self.heads.append(self.attn_head)\n",
        "        \n",
        "        self.linear_proj = nnx.Linear(in_features=self.all_head_size, out_features=self.embed_dim, use_bias=True, rngs=nnx.Rngs(0))\n",
        "        self.dropout = nnx.Dropout(0.3, rngs=nnx.Rngs(0))\n",
        "\n",
        "    def __call__(self, x: jnp.array):\n",
        "        attn_outputs = [head(x) for head in self.heads]\n",
        "\n",
        "        concat_output = jnp.concatenate(attn_outputs, axis=-1)\n",
        "        proj_output = self.linear_proj(concat_output)\n",
        "        proj_output = self.dropout(x)\n",
        "\n",
        "        return proj_output\n",
        "\n",
        "# MLP class\n",
        "class MLP(nnx.Module):\n",
        "    def __init__(self, config: dict):\n",
        "        self.embed_dim = config[\"embed_dim\"]\n",
        "        self.intermediate_size = config[\"intermediate_size\"]\n",
        "\n",
        "        self.linear1 = nnx.Linear(in_features=self.embed_dim, out_features=self.intermediate_size, use_bias=True, rngs=nnx.Rngs(0))\n",
        "        self.linear2= nnx.Linear(in_features=self.intermediate_size, out_features=self.embed_dim, use_bias=True, rngs=nnx.Rngs(0))\n",
        "        self.dropout = nnx.Dropout(0.3, rngs=nnx.Rngs(0))\n",
        "\n",
        "    def __call__(self, x: jnp.array):\n",
        "        x = self.linear1(x)\n",
        "        x = nnx.relu(x)\n",
        "        x = self.linear2(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Single block\n",
        "class Block(nnx.Module):\n",
        "    def __init__(self, config: dict):\n",
        "        self.config = config\n",
        "        self.embed_dim = config[\"embed_dim\"]\n",
        "        self.norm = nnx.BatchNorm(num_features=self.embed_dim, rngs=nnx.Rngs(0))\n",
        "        self.mha = MultiHeadAttention(self.config)\n",
        "        self.mlp = MLP(self.config)\n",
        "\n",
        "    def __call__(self, x: jnp.array):\n",
        "        norm_out = self.norm(x)\n",
        "        attn_out = self.mha(x)\n",
        "        attn_out = x + attn_out\n",
        "        norm_out = self.norm(attn_out)\n",
        "        mlp_out = self.mlp(norm_out)\n",
        "        block_out = mlp_out + norm_out\n",
        "\n",
        "        return block_out\n",
        "\n",
        "# Encoder\n",
        "class Encoder(nnx.Module):\n",
        "    def __init__(self, config: dict):\n",
        "        self.config = config\n",
        "        self.num_of_blocks = config[\"num_of_blocks\"]\n",
        "\n",
        "        self.blocks = nnx.List()\n",
        "\n",
        "        for _ in range(self.num_of_blocks):\n",
        "            block = Block(self.config)\n",
        "            self.blocks.append(block)\n",
        "\n",
        "\n",
        "    def __call__(self, x: jnp.array):\n",
        "        all_attns = []\n",
        "\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "        return x\n",
        "\n",
        "# ViT Classifier\n",
        "class ViT(nnx.Module):\n",
        "    def __init__(self, config: dict):\n",
        "        self.embed_layer = Embed(config, nnx.Rngs(0))\n",
        "        self.encoder = Encoder(config)\n",
        "        self.classifier = nnx.Linear(in_features=config[\"embed_dim\"], out_features=config[\"no_of_classes\"], use_bias=True, rngs=nnx.Rngs(0))\n",
        "\n",
        "    def __call__(self, x: jnp.array):\n",
        "        x = self.embed_layer(x)\n",
        "        x = self.encoder(x)\n",
        "        x = self.classifier(x[:, 0, :])\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## PyTorch Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FolderDataset(Dataset):\n",
        "    \"\"\"\n",
        "        A Custom dataset class inheriting the PyTorch Dataset Class.\n",
        "        Receives the the master directory as input which contains the images of different classes in different folders.\n",
        "    \"\"\"\n",
        "    def __init__(self, parent_dir: str, image_size: tuple, resize: bool, train: bool, train_size: float):\n",
        "        self.parent_dir = parent_dir\n",
        "        self.paths = list(Path(parent_dir).glob(\"*/*.jpg\"))\n",
        "        self.train = train\n",
        "        self.train_size = train_size\n",
        "        self.resize = resize\n",
        "        self.image_size = image_size\n",
        "\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.PILToTensor(),\n",
        "            transforms.Lambda(lambda x: x.permute(1, 2, 0))\n",
        "        ])\n",
        "\n",
        "        if self.train:\n",
        "            self.paths = self.paths[:int(self.train_size * len(self.paths))]\n",
        "        else:\n",
        "            self.paths = self.paths[int(self.train_size * len(self.paths)):]\n",
        "\n",
        "        self.classes, self.class_to_idx = self.get_class_class_idx()\n",
        "    \n",
        "\n",
        "    def get_class_class_idx(self):\n",
        "        \"\"\"\n",
        "            Takes in the master directory that contains the subdirectories with the images.\n",
        "        \"\"\"\n",
        "        classes = os.listdir(self.parent_dir)\n",
        "        class_idx = {}\n",
        "\n",
        "        for i, c in enumerate(classes):\n",
        "            class_idx[c] = i\n",
        "        \n",
        "        return classes, class_idx\n",
        "\n",
        "    def load_image(self, index: int):\n",
        "        image_path = self.paths[index]\n",
        "        image = Image.open(image_path)\n",
        "\n",
        "        if self.resize:\n",
        "            image = image.resize((self.image_size))\n",
        "    \n",
        "        return self.transform(image)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "\n",
        "    def __getitem__(self, index: int):\n",
        "        image = self.load_image(index)\n",
        "        class_name = self.paths[index].parent.name\n",
        "        class_idx = torch.tensor(self.class_to_idx[class_name], dtype=torch.int32)\n",
        "\n",
        "        return image, class_idx\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def loss_fn(model: nnx.Module, images: jnp.array, labels: jnp.array):\n",
        "    logits = model(images)\n",
        "    loss = optax.losses.softmax_cross_entropy_with_integer_labels(logits, labels).mean()\n",
        "    return loss, logits\n",
        "\n",
        "@nnx.jit\n",
        "def train_step(model: nnx.Module, images: jnp.array, labels: jnp.array, optim: nnx.Optimizer):\n",
        "    grad_fn = nnx.value_and_grad(loss_fn, has_aux=True)\n",
        "    (loss, logits), grads = grad_fn(model, images, labels)\n",
        "    \n",
        "    optim.update(model, grads)\n",
        "    \n",
        "    return loss\n",
        "\n",
        "@nnx.jit\n",
        "def eval_step(model: nnx.Module, images: jnp.array, labels: jnp.array, eval_metrics: nnx.MultiMetric):\n",
        "    loss, logits = loss_fn(model, images, labels)\n",
        "    eval_metrics.update(\n",
        "        loss=loss,\n",
        "        logits=logits,\n",
        "        labels=labels\n",
        "    )\n",
        "\n",
        "def train_one_epoch(model: nnx.Module, train_dataloader: DataLoader, epoch: int, epochs: int, optim: nnx.Optimizer, train_history: dict):\n",
        "    tqdm_bar_format = \"{desc}[{n_fmt}/{total_fmt}]{postfix} [{elapsed}<{remaining}]\"\n",
        "    total_steps = len(train_dataloader)\n",
        "    \n",
        "    model.train()\n",
        "\n",
        "    with tqdm.tqdm(\n",
        "        desc=f\"[Train] epoch: {epoch} / {epochs}, \",\n",
        "        total=total_steps,\n",
        "        bar_format=tqdm_bar_format,\n",
        "        leave=True\n",
        "    ) as pbar:\n",
        "        for images, labels in train_dataloader:\n",
        "            loss = train_step(model, images, labels, optim)\n",
        "            train_history[\"train_loss\"].append(loss.item())\n",
        "            pbar.set_postfix({\"loss\": loss.item()})\n",
        "            pbar.update(1)\n",
        "\n",
        "def eval_model(model: nnx.Module, val_dataloader: DataLoader, epoch: int, eval_metrics: nnx.MultiMetric, val_history: dict):\n",
        "    model.eval()\n",
        "    eval_metrics.reset() \n",
        "\n",
        "    for val_images, val_labels in val_dataloader:\n",
        "        loss = eval_step(model, val_images, val_labels, eval_metrics)\n",
        "\n",
        "    for metric, value in eval_metrics.compute().items():\n",
        "        val_history[f\"val_{metric}\"].append(value)\n",
        "    \n",
        "    print(f\"[Val] epoch: {epoch + 1} / {epochs}\")\n",
        "    print(f\"Loss: {val_history['val_loss'][-1]: .4f}\")\n",
        "    print(f\"Accuracy: {val_history['val_accuracy'][-1]: .4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Train] epoch: 0 / 100, [12/18], loss=0.000819 [00:23<00:11]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 83\u001b[39m\n\u001b[32m     81\u001b[39m \u001b[38;5;66;03m# Training loop\u001b[39;00m\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m     \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_history\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     84\u001b[39m     eval_model(model, val_dataloader, epoch, eval_metrics, val_history)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 36\u001b[39m, in \u001b[36mtrain_one_epoch\u001b[39m\u001b[34m(model, train_dataloader, epoch, epochs, optim, train_history)\u001b[39m\n\u001b[32m     28\u001b[39m model.train()\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m tqdm.tqdm(\n\u001b[32m     31\u001b[39m     desc=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[Train] epoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m / \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     32\u001b[39m     total=total_steps,\n\u001b[32m     33\u001b[39m     bar_format=tqdm_bar_format,\n\u001b[32m     34\u001b[39m     leave=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     35\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m        \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_history\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrain_loss\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Kumar\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:741\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    738\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    739\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    740\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m741\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    742\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    743\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    744\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    745\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    746\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    747\u001b[39m ):\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Kumar\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:801\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    799\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    800\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m801\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    802\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    803\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Kumar\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     52\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     53\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     56\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 52\u001b[39m, in \u001b[36mFolderDataset.__getitem__\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index: \u001b[38;5;28mint\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m     image = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     53\u001b[39m     class_name = \u001b[38;5;28mself\u001b[39m.paths[index].parent.name\n\u001b[32m     54\u001b[39m     class_idx = torch.tensor(\u001b[38;5;28mself\u001b[39m.class_to_idx[class_name], dtype=torch.int32)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 41\u001b[39m, in \u001b[36mFolderDataset.load_image\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_image\u001b[39m(\u001b[38;5;28mself\u001b[39m, index: \u001b[38;5;28mint\u001b[39m):\n\u001b[32m     40\u001b[39m     image_path = \u001b[38;5;28mself\u001b[39m.paths[index]\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m     image = \u001b[43mImage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.resize:\n\u001b[32m     44\u001b[39m         image = image.resize((\u001b[38;5;28mself\u001b[39m.image_size))\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Kumar\\anaconda3\\Lib\\site-packages\\PIL\\Image.py:3493\u001b[39m, in \u001b[36mopen\u001b[39m\u001b[34m(fp, mode, formats)\u001b[39m\n\u001b[32m   3491\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_path(fp):\n\u001b[32m   3492\u001b[39m     filename = os.fspath(fp)\n\u001b[32m-> \u001b[39m\u001b[32m3493\u001b[39m     fp = \u001b[43mbuiltins\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   3494\u001b[39m     exclusive_fp = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   3495\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    \n",
        "    # Custom collate function\n",
        "    def numpy_collate(batch):\n",
        "        return tree_map(jnp.asarray, default_collate(batch))\n",
        "\n",
        "    # Create Dataset\n",
        "    train_data = FolderDataset(\n",
        "        parent_dir=\"C:/Users/Kumar/Desktop/csiro-biomass/dataset\",\n",
        "        train=True,\n",
        "        train_size=0.8,\n",
        "        resize=True,\n",
        "        image_size=(224,224),\n",
        "    )\n",
        "    val_data = FolderDataset(\n",
        "        parent_dir=\"C:/Users/Kumar/Desktop/csiro-biomass/dataset\",\n",
        "        train=False,\n",
        "        train_size=0.8,\n",
        "        resize=True,\n",
        "        image_size=(224,224),\n",
        "    )\n",
        "\n",
        "    # Create Dataloaders\n",
        "    train_dataloader = DataLoader(\n",
        "        train_data,\n",
        "        batch_size=16,\n",
        "        shuffle=True,\n",
        "        collate_fn=numpy_collate\n",
        "    )\n",
        "\n",
        "    val_dataloader = DataLoader(\n",
        "        val_data,\n",
        "        batch_size=16,\n",
        "        shuffle=False,\n",
        "        collate_fn=numpy_collate\n",
        "    )\n",
        "\n",
        "    # Misc Settings\n",
        "    \n",
        "\n",
        "    lr_0 = 0.001\n",
        "    lr_f = 1e-5\n",
        "    momemtum = 0.8\n",
        "    epochs = 100\n",
        "    total_steps = len(train_data)\n",
        "    \n",
        "    config_dict = {\n",
        "        \"patch_size\": 16,\n",
        "        \"embed_dim\": 128,\n",
        "        \"image_size\": 224,\n",
        "        \"batch\": 16,\n",
        "        \"no_of_classes\": 10,\n",
        "        \"num_of_blocks\": 6,\n",
        "        \"num_of_heads\": 12,\n",
        "        \"intermediate_size\": 4 * 128,\n",
        "    }\n",
        "\n",
        "    train_history = {\n",
        "        \"train_loss\": [], \n",
        "    }\n",
        "\n",
        "    val_history = {\n",
        "        \"val_loss\": [],\n",
        "        \"val_accuracy\": []\n",
        "    }\n",
        "    \n",
        "    # Model Settings\n",
        "    model = ViT(config_dict)\n",
        "\n",
        "    lr_schedule = optax.linear_schedule(lr_0, lr_f, epochs * total_steps)\n",
        "\n",
        "    optim = nnx.Optimizer(\n",
        "        model, optax.adam(lr_schedule), wrt=nnx.Param\n",
        "    )\n",
        "\n",
        "    eval_metrics = nnx.MultiMetric(\n",
        "        loss = nnx.metrics.Average(\"loss\"),\n",
        "        accuracy = nnx.metrics.Accuracy(),\n",
        "    )\n",
        "    \n",
        "    # Training loop\n",
        "    for epoch in range(epochs):\n",
        "        train_one_epoch(model, train_dataloader, epoch, epochs, optim, train_history)\n",
        "        eval_model(model, val_dataloader, epoch, eval_metrics, val_history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Legacy code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the loss function\n",
        "def loss_fn(model: nnx.Module, images: jnp.array, labels: jnp.array):\n",
        "    logits = model(images)\n",
        "    loss = optax.losses.softmax_cross_entropy_with_integer_labels(logits, labels).mean()\n",
        "    return loss, logits\n",
        "\n",
        "# Define Training and validation step\n",
        "def train_step(model: nnx.Module, optim: nnx.Optimizer, images: jax.Array, labels: jax.Array):\n",
        "\n",
        "    grad_fn = nnx.value_and_grad(loss_fn, has_aux=True)\n",
        "    (loss, logits), grads = grad_fn(model, images, labels)\n",
        "    \n",
        "    optim.update(model, grads)\n",
        "    return loss\n",
        "\n",
        "def eval_step(model: nnx.Module, images: jax.Array, labels: jax.Array, eval_metrics: nnx.MultiMetric):\n",
        "    \n",
        "    loss, logits = loss_fn(model, images, labels)\n",
        "    eval_metrics.update(\n",
        "        loss=loss,\n",
        "        logits=logits,\n",
        "        labels=labels\n",
        "    )\n",
        "\n",
        "train_step = nnx.jit(train_step)\n",
        "eval_step = nnx.jit(eval_step)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "eval_metrics = nnx.MultiMetric(\n",
        "    loss = nnx.metrics.Average(\"loss\"),\n",
        "    accuracy = nnx.metrics.Accuracy(),\n",
        ")\n",
        "\n",
        "train_history = {\n",
        "    \"train_loss\": [], \n",
        "}\n",
        "\n",
        "val_history = {\n",
        "    \"val_loss\": [],\n",
        "    \"val_accuracy\": []\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure optimizer, loss, metrics\n",
        "\n",
        "tqdm_bar_format = \"{desc}[{n_fmt}/{total_fmt}]{postfix} [{elapsed}<{remaining}]\"\n",
        "\n",
        "lr_0 = 0.001\n",
        "lr_f = 1e-5\n",
        "momemtum = 0.8\n",
        "epochs = 100\n",
        "total_steps = x_train.shape[0] // 16\n",
        "\n",
        "config_dict = {\n",
        "    \"patch_size\": 4,\n",
        "    \"embed_dim\": 128,\n",
        "    \"image_size\": 32,\n",
        "    \"batch\": 16,\n",
        "    \"no_of_classes\": 10,\n",
        "    \"num_of_blocks\": 6,\n",
        "    \"num_of_heads\": 12,\n",
        "    \"intermediate_size\": 4 * 128,\n",
        "}\n",
        "train_history = {\n",
        "    \"train_loss\": [], \n",
        "}\n",
        "\n",
        "\n",
        "model = ViT(config_dict)\n",
        "\n",
        "lr_schedule = optax.linear_schedule(lr_0, lr_f, epochs * total_steps)\n",
        "\n",
        "optim = nnx.Optimizer(\n",
        "    model, optax.adam(lr_schedule), wrt=nnx.Param\n",
        ")\n",
        "\n",
        "def train_one_epoch(epoch: int):\n",
        "    model.train()\n",
        "\n",
        "    with tqdm.tqdm(\n",
        "        desc=f\"[Train] epoch: {epoch} / {epochs}, \",\n",
        "        total=total_steps,\n",
        "        bar_format=tqdm_bar_format,\n",
        "        leave=True\n",
        "    ) as pbar:\n",
        "        for images, labels in next(train_dataloader):\n",
        "            loss = train_step(model, optim, images, labels)\n",
        "            train_history[\"train_loss\"].append(loss.item())\n",
        "            pbar.set_postfix({\"loss\": loss.item()})\n",
        "            pbar.update(1)\n",
        "    \n",
        "def eval_model(epoch: int):\n",
        "    model.eval()\n",
        "    eval_metrics.reset()\n",
        "\n",
        "    for val_images, val_labels in next(val_dataloader):\n",
        "        loss = eval_step(model, val_images, val_labels, eval_metrics)\n",
        "    \n",
        "    for metric, value in eval_metrics.compute().items():\n",
        "        val_history[f\"val_{metric}\"].append(value)\n",
        "\n",
        "    print(f\"[Val] epoch: {epoch + 1} / {epochs}\")\n",
        "    print(f\"Loss: {val_history['val_loss'][-1]:0.4f}\")\n",
        "    print(f\"Accuracy: {val_history['val_accuracy'][-1]:0.4f}\")\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "625"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(train_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for epoch in range(epochs):\n",
        "    train_one_epoch(epoch)\n",
        "    eval_model(epoch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
